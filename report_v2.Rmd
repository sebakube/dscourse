---
title: 'Data Science: Capstone - Movielens project'
output: 
  pdf_document:
    fig_caption: true
    fig_width: 5
    fig_height: 3
---
```{r libraries, eval=TRUE, echo=FALSE,include=FALSE,message = FALSE,warning=FALSE}
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(data.table)) install.packages("data.table", repos = "http://cran.us.r-project.org")
if(!require(readr)) install.packages("readr", repos = "http://cran.us.r-project.org")
if(!require(stringr)) install.packages("stringr", repos = "http://cran.us.r-project.org")
if(!require(lubridate)) install.packages("lubridate", repos = "http://cran.us.r-project.org")
if(!require(dplyr)) install.packages("dplyr", repos = "http://cran.us.r-project.org")

library(tidyverse)
library(caret)
library(data.table)
library(readr)
library(lubridate)
library(stringr)
library(dplyr)
```
# Introduction

The [Movielens 10M dataset](https://grouplens.org/datasets/movielens/10m/) is a dataset of movie recommendations provided by the [grouplens](https://grouplens.org/) lab of the University of Minnesota. According to its readme file it consists of 10000054 ratings of 10681 movies by 71567 users from the online movie recommender service MovieLens. User reviews are anonymized. The user is represented by an identifier (ID) and review submission is time-stamped. Movies are represented by a unique movieID, their title together with the year of publication, and one or more different genre tags. Ratings are given on a scale from 0 to 5 stars in 0.5 star increments. In the present work, a generalized linear model combined with elastic net regularization was used to predict movie ratings. The final model was trained on 90 % of the data with 5-fold cross-validation. It uses bias factors taking characteristics of the movie (year of publication, genre and individual movie bias) and of the user (numbers of years the review was submitted, individual user bias, submission week of review) into account, as well as the total number of ratings a movie has received and the temporal order in which a user submits the reviews. The proposed model yields a RMSE of 0.86452 on an independent validation dataset.

# Analysis

This section describes the preparation of the dataset and initial exploratory analysis to identify baseline predictors for the modeling approach.

## Dataset preparation
The [Movielens 10M dataset](https://grouplens.org/datasets/movielens/10m/) is downloaded and partitioned into evaluation and validation data. The prediction method is developed based only on the evaluation data. To optimize the prediction method, the training data is further partitioned into a training and training-test data set (80 % training, 20 % test). The final prediction method is re-trained on the full evaluation dataset prior to validation.

### Download and partition of Movielens 10M Dataset
The Movielens 10M dataset is partioned into a dataset for evaluation *edx* (90 % of the data) and a final hold-out dataset for validation *validation* (10 % of the data). The procedure is performed as defined in the course instructions. *edx* and *valiation* are exported to csv files.

```{r Import, eval=FALSE, echo=FALSE, include=FALSE,message = FALSE,warning=FALSE}
##########################################################
# Create edx set, validation set (final hold-out test set)
##########################################################

# Note: this process could take a couple of minutes



# MovieLens 10M dataset:
# https://grouplens.org/datasets/movielens/10m/
# http://files.grouplens.org/datasets/movielens/ml-10m.zip

dl <- tempfile()
download.file("http://files.grouplens.org/datasets/movielens/ml-10m.zip", dl)

ratings <- fread(text = gsub("::", "\t", readLines(unzip(dl, "ml-10M100K/ratings.dat"))),
                 col.names = c("userId", "movieId", "rating", "timestamp"))

movies <- str_split_fixed(readLines(unzip(dl, "ml-10M100K/movies.dat")), "\\::", 3)
colnames(movies) <- c("movieId", "title", "genres")

# if using R 4.0 or later:
movies <- as.data.frame(movies) %>% mutate(movieId = as.numeric(movieId),
                                           title = as.character(title),
                                           genres = as.character(genres))


movielens <- left_join(ratings, movies, by = "movieId")

# Validation set will be 10% of MovieLens data
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = movielens$rating, times = 1, p = 0.1, list = FALSE)
edx <- movielens[-test_index,]
temp <- movielens[test_index,]

# Make sure userId and movieId in validation set are also in edx set
validation <- temp %>% 
  semi_join(edx, by = "movieId") %>%
  semi_join(edx, by = "userId")

# Add rows removed from validation set back into edx set
removed <- anti_join(temp, validation)
edx <- rbind(edx, removed)

rm(dl, ratings, movies, test_index, temp, movielens, removed)

write_csv(edx, "edx.csv")
write_csv(validation, "validation.csv")
```

### Data pre-processing
Inspection of the evaluation data shows that the publication year of the movie can be extracted from the title using the *stringr* package. Extraction is realized in two parts to safely distinguish the publication year given in brackets from any other four-digit number in the movie title. The publication year is stored in *pub_year*. The timestamp of the user review is converted to a date and stored in *review_date*. The new dataset is called *edx_work*. All operations need to be applied to the *validation* data set prior to validation of the final model.

Prior to pre-processing the dataset looks like this:
```{r load_edx, eval=TRUE,echo=FALSE,include=FALSE,message = FALSE,warning=FALSE}
edx <- read_csv("edx.csv")

```

```{r Preview_edx,eval=TRUE,echo=FALSE,include=TRUE,message = FALSE,warning=FALSE}
head(edx)
```

```{r data_wrangling_edx,eval=TRUE,echo=FALSE,include=TRUE,message = FALSE,warning=FALSE}
#prepares dataset edx for exploratory data analysis, needs to be applied to validation later on 

# convert timestamp to review_date, extract publication year (pub_year) from title, separate by genre
# 2-step extraction of pub_year to avoid mix-up when a date year is part of the actual movie title
pattern1 <- "\\(\\d{4}\\)"
pattern2 <-"\\d{4}"

edx_work <- edx %>% 
  mutate(review_date = as_datetime(timestamp),
         pub_year=as.numeric(str_match(str_match(title,pattern1),pattern2)))

write_csv(edx_work, "data/edx_work.csv")
```

After pre-processing it looks like this:

```{r load_edx_work, eval=TRUE,echo=FALSE,include=FALSE,message = FALSE,warning=FALSE}
edx_work <- read_csv("edx_work.csv")

```

```{r Preview_edx_work,eval=TRUE,echo=FALSE,include=TRUE,message = FALSE,warning=FALSE}
head(edx_work)
```

### Partition of evaluation dataset
The *edx_work* dataset is further divided into sets *edx_work_test* (20%) and *edx_work_train* (80%) using the procedure provided for defining *edx* and *validation* data sets. The following data exploration and intermediate model training is based on *edx_work_train*.

```{r partition_edx_work,eval=FALSE, echo=FALSE, include=FALSE,message = FALSE,warning=FALSE}
# In this script, the edx_work dataset is 
# further divided into set edx_work_test (20%) and edx_work_train (80%) using the procedure
# provided for defining edx and validation data sets

# edx_test set will be 20% of edx data
set.seed(1, sample.kind="Rounding")
test_index <- createDataPartition(y = edx_work$rating, times = 1, p = 0.2, list = FALSE)
edx_work_train <- edx_work[-test_index,]
temp <- edx_work[test_index,]

# Make sure userId and movieId in edx_test set are also in edx_train
edx_work_test <- temp %>% 
  semi_join(edx_work_train, by = "movieId") %>%
  semi_join(edx_work_train, by = "userId")

# Add rows removed from edx_test set back into edx_train set
removed <- anti_join(temp, edx_work_test)
edx_work_train <- rbind(edx_work_train, removed)

rm(test_index, temp, removed)
write_csv(edx_work_train, "edx_work_train.csv")
write_csv(edx_work_test, "edx_work_test.csv")
```

## Data exploration
In the *edx_work_train* data set, there are 10677 different movies reviewed by 69878 users. The average rating is approx. 3.51 with a standard deviation of approx. 1.06. Average and standard deviation are used as reference to assess the effect of different variables on the rating of a movie.

```{r data_exploration_1,eval=TRUE,echo=TRUE,include=FALSE,message = FALSE,warning=FALSE}
## load datasets
edx_work_train <- read_csv("edx_work_train.csv")
edx_work_test <- read_csv("edx_work_test.csv")
## dataset description
mu <- mean(edx_work_train$rating)
sd <- sd(edx_work_train$rating)
mu
sd
n_distinct(edx_work_train$userId) #number of different users
n_distinct(edx_work_train$movieId) # number of different movies
```

### Movie effects
In this section, base line predictors are explored that are associated with a specific movie.

#### Publication year of movie
The mean movie rating per publication year is calculated.
```{r data_exploration_2, eval=TRUE,echo=FALSE,include=FALSE,message = FALSE,warning=FALSE}

#pub_year effects are analysed by calcutlating the mean rating for every publication year in the data.
pub_year_effects <- edx_work_train %>% 
  group_by(pub_year) %>%
  summarize(n_ratings=n(),avg_rating=mean(rating))

fig_1 <- ggplot(data=pub_year_effects,aes(pub_year,n_ratings))+geom_point() + 
  xlab("publication year") + 
  ylab("total number of ratings") +
  theme_bw()

fig_2 <- ggplot(data=pub_year_effects,aes(pub_year,avg_rating))+geom_point() + 
  xlab("publication year") +
  ylab("average rating") +
  geom_hline(yintercept = mu,colour ="red") +
  geom_hline(yintercept = mu+sd,colour ="red",linetype="33") +
  geom_hline(yintercept = mu-sd,colour ="red",linetype="33") + 
  annotate("text", x = 1920, y = mu + 0.1, label = "AVG", color="red") + 
  annotate("text", x = 1920, y = mu + sd - 0.1, label = "+1 SD", color="red") +
  annotate("text", x = 1920, y = mu - sd + 0.1, label = "-1 SD", color="red") +
  theme_bw()
```

The dataset *edx_work train* includes movies published from 1915 to 2008. Fig. 1 shows the number of ratings per publication year. While the number of ratings for movies published before 1950 is low, it steadily increases afterwards with a maximum in 1995 indicating a need for regularization when this bias is included into a model. The average rating of a movie with a publication year earlier than 1984 is above average, while afterwards it approaches the overall average (Fig. 2, red line).

```{r figures1, eval=TRUE,echo=TRUE,message = FALSE,warning=FALSE,fig.cap=c("number of ratings per publication year", "average rating per publication year")}
fig_1

fig_2
```

#### Movie genre

```{r data_exploration_3, eval=TRUE,include=FALSE,message = FALSE,warning=FALSE}
genre_effects <- edx_work_train %>% 
  group_by(genres) %>% 
  summarize(n_ratings=n(),genre_avg=mean(rating))

fig_3 <- ggplot(data=genre_effects,aes(genres,n_ratings))+geom_point() + 
  scale_y_log10() +
  xlab("genre/sub-genre") + 
  ylab("log10 total number of ratings") +
  theme_bw() +
  theme(axis.text.x=element_blank(),axis.ticks.x=element_blank())

fig_4 <- ggplot(data=genre_effects,aes(reorder(genres, -genre_avg, sum),y=genre_avg))+geom_bar(stat="identity")  +
  xlab("genre/sub-genre") +
  ylab("average rating") +
  geom_hline(yintercept = mu,colour ="red") +
  geom_hline(yintercept = mu+sd,colour ="red",linetype="33") +
  geom_hline(yintercept = mu-sd,colour ="red",linetype="33") +
  annotate("text", x = "Children", y = mu +0.2, label = "AVG", color="red", size = 4 ) + 
  annotate("text", x = "Children", y = mu + sd - 0.2, label = "+1 SD", color="red", size = 4 ) +
  annotate("text", x = "Children", y = mu - sd + 0.2, label = "-1 SD", color="red", size = 4 ) +
  theme_bw() +
  theme(axis.text.x=element_blank(),axis.ticks.x=element_blank())
```

The dataset *edx_work train* contains 10677 movies of 797 genres and sub-genres. The number of ratings are not evenly distributed (Fig. 3) indicating a need for regularization when this bias is included in to a model. The average rating per genre/sub-genre is shown in Fig. 4 demonstrating an effect on the movie rating.

```{r figures2, eval=TRUE,echo=TRUE,message = FALSE,warning=FALSE,fig.cap=c("number of ratings per genre/sub-genre","average rating by genre/sub-genre")}
fig_3
fig_4
```


#### Number of ratings

Next, the number of ratings per movie was investigated whether it affects the movie rating. For this, the number of distinct user ID associated to a movie ID was counted and stratified to multiples of hundred. The number of ratings per group are shown in Fig. 5. Fig. 6 demonstrates that the average rating generally increases with an increasing number of ratings.

```{r data_exploration_4, eval=TRUE,echo=FALSE,include=TRUE,message = FALSE,warning=FALSE}
nr_effects <- edx_work_train %>% 
  group_by(movieId) %>% 
  summarize(n_rating_round=round(n_distinct(userId)/100)) %>%
  left_join(x=edx_work_train,y=.,by="movieId") %>%
  group_by(n_rating_round) %>%
  summarize(n_ratings=n(),avg_rating=mean(rating))

fig_5 <- ggplot(data=nr_effects,aes(n_rating_round,n_ratings))+geom_point() + 
  xlab("number of ratings in 100s") + 
  ylab("total number of ratings") +
  theme_bw() 

fig_6 <- ggplot(data=nr_effects,aes(n_rating_round,avg_rating))+geom_point() + 
  xlab("number of ratings in 100s") +
  ylab("average rating") +
  geom_hline(yintercept = mu,colour ="red") +
  geom_hline(yintercept = mu+sd,colour ="red",linetype="33") +
  geom_hline(yintercept = mu-sd,colour ="red",linetype="33") +
  annotate("text", x = 0.1, y = mu +0.05, label = "AVG", color="red") + 
  annotate("text", x = 0.1, y = mu + sd - 0.05, label = "+1 SD", color="red") +
  annotate("text", x = 0.1, y = mu - sd + 0.05, label = "-1 SD", color="red")
```



```{r figures3, eval=TRUE,echo=TRUE,message = FALSE,warning=FALSE,fig.cap=c("number of ratings per number of ratings in 100s","average rating by number of ratings in 100s")}
fig_5
fig_6
```

### User effects

In this section, base line predictors are explored that are associated with a specific user or user review.

#### Number of reviews per user

First, the number of ratings per user was investigated whether it affects the movie rating. For this, the number of distinct movie IDs associated to a user ID was counted and stratified to multiples of ten. The number of individual ratings per group are shown in Fig. 7. The biggest user group has only rated between 15 to 24 individual movies. The size of user groups with more ratings is rapidly declining. Fig. 8 demonstrates that the average rating generally decreases with an increasing number of ratings.

```{r data_exploration_5, eval=TRUE,echo=FALSE,include=TRUE,message = FALSE,warning=FALSE}
#review frequency of user, stratification to multiples of ten
rpu_effects <- edx_work_train %>% group_by(userId) %>% 
  mutate(reviews_per_user =round(n_distinct(movieId)/10)) %>% 
  group_by(reviews_per_user) %>% 
  summarize(n_ratings=n(), rpu_avg = mean(rating),n_users=n_distinct(userId))

slice_max(rpu_effects,n_ratings) # give the user group with the most ratings
slice_max(rpu_effects,n_users) # give the biggest user group

fig_7 <- ggplot(data=rpu_effects,aes(reviews_per_user,n_ratings))+geom_point() + 
  xlab("user group (reviews per user in 10s)") + 
  ylab("number of rated movies") +
  theme_bw() 

fig_8 <- ggplot(data=rpu_effects,aes(reviews_per_user,rpu_avg))+geom_point() + 
  xlab("user group (reviews per user in 10s)") +
  ylab("average rating") +
  geom_hline(yintercept = mu,colour ="red") +
  geom_hline(yintercept = mu+sd,colour ="red",linetype="33") +
  geom_hline(yintercept = mu-sd,colour ="red",linetype="33") +
  annotate("text", x = 40, y = mu +0.1, label = "AVG", color="red") + 
  annotate("text", x = 40, y = mu + sd - 0.1, label = "+1 SD", color="red") +
  annotate("text", x = 40, y = mu - sd + 0.1, label = "-1 SD", color="red") +
  theme_bw() 
```

```{r figures4, eval=TRUE,echo=TRUE,message = FALSE,warning=FALSE,fig.cap=c("number of reviewed movies per user group (number of reviews in 10s)","average rating per per user group (number of reviews in 10s)")}
fig_7
fig_8
```

The number of reviews increases over time as the user submits more and more reviews which means that a possible bias changes. This is investigated further. To do this, the review_date is stratified to weeks and assigned an order for a specific user. The average rating is calculated for a review week (Fig. 9) and the review week order counted from the first week when a review was submitted by a specific user (Fig. 10). While the average for the review week is fluctuating around the overall mean rating, the average rating is slightly below the overall average rating with increasing review order.

```{r data_exploration_6, eval=TRUE,echo=FALSE,include=TRUE,message = FALSE,warning=FALSE}
# round the review_date to weeks and assign an order for a specific user Id and review week
review_order_train <- edx_work_train %>%
  mutate(review_week=round_date(review_date,"week")) %>%
  select(userId,review_week) %>%
  distinct() %>%
  group_by(userId) %>%
  summarize(review_week,review_order=order(review_week))

# join review_week and review_order into edx_work_train to be able to plot time courses
time_course <- edx_work_train %>%
  mutate(review_week=round_date(review_date,"week")) %>%
  left_join(x=.,y=review_order_train, by=c("userId","review_week"))
# calculate average rating per review week
review_week_effects <- time_course %>% group_by(review_week) %>%
  summarise(n_ratings=n(),avg_rating=mean(rating))
# calculate average rating per review order
review_order_effects <- time_course %>% group_by(review_order) %>%
  summarise(n_ratings=n(),avg_rating=mean(rating))

fig_9 <- ggplot(data=review_week_effects,aes(review_week,avg_rating))+geom_point() + 
  xlab("review week") +
  ylab("average rating") +
  geom_hline(yintercept = mu,colour ="red") +
  geom_hline(yintercept = mu+sd,colour ="red",linetype="33") +
  geom_hline(yintercept = mu-sd,colour ="red",linetype="33") +
  annotate("text", x=as.POSIXct("1995-03-31 00:00:00"), y = mu +0.1, label = "AVG", color="red") + 
  annotate("text", x=as.POSIXct("1995-03-31 00:00:00"), y = mu + sd - 0.1, label = "+1 SD", color="red") +
  annotate("text", x=as.POSIXct("1995-03-31 00:00:00"), y = mu - sd + 0.1, label = "-1 SD", color="red") +
  theme_bw()

fig_10 <- ggplot(data=review_order_effects,aes(review_order,avg_rating))+geom_point() + 
  xlab("order of review week") +
  ylab("average rating") +
  geom_hline(yintercept = mu,colour ="red") +
  geom_hline(yintercept = mu+sd,colour ="red",linetype="33") +
  geom_hline(yintercept = mu-sd,colour ="red",linetype="33") +
  annotate("text", x=5, y = mu +0.1, label = "AVG", color="red") + 
  annotate("text", x=5, y = mu + sd - 0.1, label = "+1 SD", color="red") +
  annotate("text", x=5, y = mu - sd + 0.1, label = "-1 SD", color="red") +
  theme_bw()
```

```{r figures5, eval=TRUE,echo=TRUE,message = FALSE,warning=FALSE,fig.cap=c("time course in weeks of average rating","average rating in ordered review week")}
fig_9
fig_10
```

#### Timespan in years between review_date and the year of publication

Then, the number of years *t* after the publication year was investigated whether it affects the movie rating. For this, the publication year was subtracted from the review year. The number of ratings per *t* are shown in Fig. 11. Fig. 12 demonstrates that the average rating generally increases to a plateau (*t* approx. 30 years) with a little bump at around 10 years after movie publication.

```{r data_exploration_7, eval=TRUE,echo=FALSE,include=TRUE,message = FALSE,warning=FALSE}

#timespan review_date - pub_year t in years 
t_effects <- edx_work_train %>% 
  mutate(t= year(round_date(review_date, "year")) - pub_year) %>% 
  group_by(t) %>% 
  summarize(n_ratings=n(),t_avg = mean(rating), b_t=mean(rating-mu))

fig_11 <- ggplot(data=t_effects,aes(t,n_ratings))+geom_point() + 
  xlab("t years after publication") + 
  ylab("total number of ratings") +
  theme_bw() 

fig_12 <- ggplot(data=t_effects,aes(t,t_avg))+geom_point() + 
  xlab("years after publication") +
  ylab("average rating") +
  geom_hline(yintercept = mu,colour ="red") +
  geom_hline(yintercept = mu+sd,colour ="red",linetype="33") +
  geom_hline(yintercept = mu-sd,colour ="red",linetype="33") +
  annotate("text", x = 0.1, y = mu +0.1, label = "AVG", color="red") + 
  annotate("text", x = 0.1, y = mu + sd - 0.1, label = "+1 SD", color="red") +
  annotate("text", x = 0.1, y = mu - sd + 0.1, label = "-1 SD", color="red") +
  theme_bw() 

```

```{r figures6, eval=TRUE,echo=TRUE,include=TRUE,message = FALSE,warning=FALSE,fig.cap=c("number of ratings t years after publication","average rating t years after publication")}
fig_11
fig_12
```

## Modelling

In this section, the creation of the model is described.

### Definition of bias parameters

In this section, the process to the final prediction model is described. In a first step, bias parameters associated to a specific movie are calculated: the effect of the publication year (*b~py~*), the effect of the genre (*b~g~*), and the movie bias (*b~i~*).

```{r movie_effects, eval=TRUE,echo=TRUE,include=TRUE,message = FALSE,warning=FALSE}
#Modelling of movie effects

# genre bias
  genre_bias <- edx_work_train %>%
    group_by(genres) %>%
    summarize(b_g=mean(rating-mu)) %>%
    ungroup()

# pub year bias
  pub_year_bias <- edx_work_train %>%
    left_join(x.,y=genre_bias,by="genres") %>%
    group_by(pub_year) %>%
    summarize(b_py=mean(rating-mu-b_g)) %>%
    ungroup()

# movie bias
  movie_bias <- edx_work_train %>%
    left_join(x.,y=genre_bias,by="genres") %>%
    left_join(x=.,y=pub_year_bias,by="pub_year") %>%
    group_by(movieId) %>%
    summarize(b_i=mean(rating-mu-b_g-b_py),n_rating=n_distinct(userId)) %>%
    ungroup()
  
fig_13 <- ggplot(data=genre_bias,aes(b_g))+geom_histogram(binwidth = 0.1) + 
  xlab("genre bias") +
  ylab("frequency") +
  theme_bw()
fig_14 <- ggplot(data=pub_year_bias,aes(b_py))+geom_histogram(binwidth = 0.1)  + 
  xlab("publication year bias") +
  ylab("frequency") +
  theme_bw() 
fig_15 <- ggplot(data=movie_bias,aes(b_i))+geom_histogram(binwidth = 0.1)  + 
  xlab("true movie bias") +
  ylab("frequency") +
  theme_bw() 
```

The distribution of these parameters are shown in Fig. 13, Fig. 14, and Fig. 15.

```{r figures7, eval=TRUE,echo=TRUE,message = FALSE,warning=FALSE,fig.cap=c("distribution of genre bias","distribution of publication year bias", "distribution of movie bias")}
fig_13
fig_14
fig_15
```

Next, bias parameters associated to a specific user are calculated: the effect of t, the number of years after the publication year a review was submitted (*b~t~*), the bias of the week the review was submitted (*b~w~*), and the effect of the user (*b~u~*). They were substracted from the residual rating which could not be explained by the movie-associated effects as defined above.

```{r user_effect, eval=TRUE,echo=TRUE,include=TRUE,message = FALSE,warning=FALSE}
#Modelling of user effects

# user bias
  user_bias <- edx_work_train %>%
    left_join(x.,y=genre_bias,by="genres") %>%
    left_join(x=.,y=pub_year_bias,by="pub_year") %>%
    left_join(x=.,y=movie_bias,by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u=mean(rating-mu-b_g-b_py-b_i)) %>%
    ungroup()

# review week bias
  review_week_bias <- edx_work_train %>% 
    mutate(t= year(round_date(review_date, "year")) - 
             pub_year,review_week=round_date(review_date,"week")) %>%
    left_join(x.,y=genre_bias,by="genres") %>%
    left_join(x=.,y=pub_year_bias,by="pub_year") %>%
    left_join(x=.,y=movie_bias,by="movieId") %>%
    left_join(x=.,y=user_bias,by="userId") %>%
    group_by(review_week) %>%
    summarize(b_w=mean(rating-mu-b_g-b_py-b_i-b_u)) %>%
    ungroup()
  
# t bias
  t_bias <- edx_work_train %>% 
    mutate(t= year(round_date(review_date, "year")) - 
             pub_year,review_week=round_date(review_date,"week")) %>%
    left_join(x.,y=genre_bias,by="genres") %>%
    left_join(x=.,y=pub_year_bias,by="pub_year") %>%
    left_join(x=.,y=movie_bias,by="movieId") %>%
    left_join(x=.,y=user_bias,by="userId") %>%
    left_join(x=.,y=review_week_bias,by="review_week") %>%
    group_by(t) %>%
    summarize(b_t=mean(rating-mu-b_g-b_py-b_i-b_u-b_w)) %>%
    ungroup()
  
fig_16 <- ggplot(data=t_bias,aes(b_t))+geom_histogram(binwidth = 0.1)  + 
  xlab("t bias") +
  ylab("frequency") +
  theme_bw()

fig_17 <- ggplot(data=user_bias,aes(b_u))+geom_histogram(binwidth = 0.1)  + 
  xlab("user bias") +
  ylab("frequency") +
  theme_bw()

fig_18 <- ggplot(data=review_week_bias,aes(b_w))+geom_histogram(binwidth = 0.1)  + 
  xlab("review week bias") +
  ylab("frequency") +
  theme_bw()
```

The distributions of user-associated parameters are shown in Fig. 16, Fig. 17, and Fig. 18.

```{r figures8, eval=TRUE,echo=TRUE,message = FALSE,warning=FALSE,fig.cap=c("distribution of t bias", "distribution of user bias", "distribution of review week bias")}
fig_16
fig_17
fig_18
```

### Preliminary models

To combine the parameters and predict ratings generalized linear models with elastic net regularization were chosen. Elastic net regularization is a combination of Lasso (least absolute shrinkage and selection operator) and Ridge regularization ([glmnet](https://cran.r-project.org/web/packages/glmnet/vignettes/glmnet.pdf)). In Lasso regularization the absolute value of estimated weights multiplied by a tuning parameter *lambda* is added to the residual sum squares term to evaluate a fit, while in Ridge regression regression it is the squared weights multiplied by *lambda*. The mixture between Lasso and rigde is controlled by the tuning parameter *alpha* ranging between 0 and 1. Tuning parameters are selected by 5-fold cross-validation. Biases are used as defined above, addtionally the user specific review order and the number of ratings a movie has received are also included.
Four models are compared, *eln_model_1* only contains the bias as defined previously, while *eln_model_2* also takes additional information into account: the order of reviews (stratified by weeks) a user has submitted, the number of ratings a movie has received in total is included. *eln_model_3* excludes the bias of review week and *eln_model_4* uses the review week directly as an additional variable.

```{r eln_model,eval=TRUE,include=TRUE,message = FALSE,warning=FALSE}
## define the settings for cross-validation.
ctrl_pars <- trainControl(method="cv", number=5)

#define the training dataset:

# define the user-specific review order
  review_order_train <- edx_work_train %>%
    mutate(review_week=round_date(review_date,"week")) %>%
    select(userId,review_week) %>%
    distinct() %>%
    group_by(userId) %>%
    summarize(review_week,review_order=order(review_week)) %>%
    ungroup()

# join to data set
  edx_work_train_order <- edx_work_train %>% 
    mutate(t= year(round_date(review_date, "year")) - 
             pub_year,review_week=round_date(review_date,"week")) %>%
    mutate(review_week=round_date(review_date,"week")) %>%
    left_join(x=.,y=review_order_train, by=c("userId","review_week"))

# complete the dataset
  edx_work_train_eln <- edx_work_train_order %>%
    left_join(x=.,y=movie_bias,by="movieId") %>%
    left_join(x=.,y=genre_bias,by="genres") %>%
    left_join(x=.,y=pub_year_bias,by="pub_year") %>%
    left_join(x=.,y=user_bias,by="userId") %>%
    left_join(x=.,y=review_week_bias,by="review_week") %>%
    left_join(x=.,y=t_bias,by="t") %>%
    select(rating,movieId,userId,b_g,b_py,b_i,b_u,b_w,b_t,
           n_rating,review_order,review_week)


#define the test data set

# define the user-specific review order
  review_order_test <- edx_work_test %>%
    mutate(review_week=round_date(review_date,"week")) %>%
    select(userId,review_week) %>%
    distinct() %>%
    group_by(userId) %>%
    summarize(review_week,review_order=order(review_week)) %>%
    ungroup()

# join to data set
  edx_work_test_order <- edx_work_test %>% 
    mutate(t= year(round_date(review_date, "year")) - 
             pub_year,review_week=round_date(review_date,"week")) %>%
    mutate(review_week=round_date(review_date,"week")) %>%
    left_join(x=.,y=review_order_test, by=c("userId","review_week"))

# complete the dataset
  edx_work_test_eln <- edx_work_test_order %>%
    left_join(x=.,y=movie_bias,by="movieId") %>%
    left_join(x=.,y=genre_bias,by="genres") %>%
    left_join(x=.,y=pub_year_bias,by="pub_year") %>%
    left_join(x=.,y=user_bias,by="userId") %>%
    left_join(x=.,y=review_week_bias,by="review_week") %>%
    left_join(x=.,y=t_bias,by="t") %>%
    select(rating,movieId,userId,b_g,b_py,b_i,b_u,b_w,b_t,
           n_rating,review_order,review_week)

#building the models

set.seed(1, sample.kind="Rounding")
eln_model_1 <- train(rating ~ b_py + b_g + b_i + b_t + b_u + b_w,
                   data = edx_work_train_eln,
                   method = "glmnet",
                   trControl = ctrl_pars,
                   metric = "RMSE",
                   family="gaussian",
                   tuneGrid = expand.grid(alpha =seq(0,1,0.33),
                                          lambda = seq(0.0001,1,length =100)))

set.seed(1, sample.kind="Rounding")
eln_model_2 <- train(rating ~ b_g + b_py + b_i + b_u + b_t + b_w + n_rating + review_order,
                   data = edx_work_train_eln,
                   method = "glmnet",
                   trControl = ctrl_pars,
                   metric = "RMSE",
                   family="gaussian",
                   tuneGrid = expand.grid(alpha =seq(0,1,0.33),
                                          lambda = seq(0.0001,1,length =100)))

set.seed(1, sample.kind="Rounding")
eln_model_3 <- train(rating ~ b_g + b_py + b_i + b_u + b_t + n_rating + review_order,
                   data = edx_work_train_eln,
                   method = "glmnet",
                   trControl = ctrl_pars,
                   metric = "RMSE",
                   family="gaussian",
                   tuneGrid = expand.grid(alpha =seq(0,1,0.33),
                                          lambda = seq(0.0001,1,length =100)))

set.seed(1, sample.kind="Rounding")
eln_model_4 <- train(rating ~ b_g + b_py + b_i + b_u + b_t + n_rating + review_order + review_week,
                   data = edx_work_train_eln,
                   method = "glmnet",
                   trControl = ctrl_pars,
                   metric = "RMSE",
                   family="gaussian",
                   tuneGrid = expand.grid(alpha =seq(0,1,0.33),
                                          lambda = seq(0.0001,1,length =100)))


#making the predictions

eln_predict_1 <- predict(eln_model_1,newdata = edx_work_test_eln, na.action=na.pass)

eln_predict_2 <- predict(eln_model_2,newdata = edx_work_test_eln, na.action=na.pass)

eln_predict_3 <- predict(eln_model_3,newdata = edx_work_test_eln, na.action=na.pass)

eln_predict_4 <- predict(eln_model_4,newdata = edx_work_test_eln, na.action=na.pass)


# replace 'NA' in prediction with mu

eln_predict_1 <- tibble(eln_predict_1) %>% 
  mutate(eln_predict_1 = ifelse(is.na(eln_predict_1) == TRUE, mu, eln_predict_1))

eln_predict_2 <- tibble(eln_predict_2) %>% 
  mutate(eln_predict_2 = ifelse(is.na(eln_predict_2) == TRUE, mu, eln_predict_2))

eln_predict_3 <- tibble(eln_predict_3) %>% 
  mutate(eln_predict_3 = ifelse(is.na(eln_predict_3) == TRUE, mu, eln_predict_3))

eln_predict_4 <- tibble(eln_predict_4) %>% 
  mutate(eln_predict_4 = ifelse(is.na(eln_predict_4) == TRUE, mu, eln_predict_4))

# calculate RMSE

rmse_1 <- RMSE(edx_work_test_eln$rating,eln_predict_1$eln_predict_1)

rmse_2 <- RMSE(edx_work_test_eln$rating,eln_predict_2$eln_predict_2)

rmse_3 <- RMSE(edx_work_test_eln$rating,eln_predict_3$eln_predict_3)

rmse_4 <- RMSE(edx_work_test_eln$rating,eln_predict_4$eln_predict_4)

```

The final model is selected based on the performance as measured by comparing the RMSE tested on an independent test set *edx_work_test*. R-squared in all models indicates that just about one third of the variance in the data is explained by the models. *eln_model_2* is selected, demonstrating the lowest mean RMSE, highest mean R-squared, as well as the lowest RMSE with respect to the independent test data set *edx_work_test*. 

```{r model_selection, eval=TRUE,echo=TRUE,message = FALSE,warning=FALSE}
# print rmses
rmse_1
rmse_2
rmse_3
rmse_4

# comparison of model performance (cross-validation)
model_list <- list(eln_model_1 = eln_model_1,
                   eln_model_2 = eln_model_2,
                   eln_model_3 = eln_model_3,
                   eln_model_4 = eln_model_4)

res <- resamples(model_list)
summary(res)



```

A final model based on *eln_model_2 *is trained on the complete *edx* dataset and tested on the *validation* dataset.

```{r final_model, eval=TRUE,echo=FALSE,include=TRUE,message = FALSE,warning=FALSE}
# calculate bias for edx dataset

# genre bias
  genre_bias <- edx_work %>%
    group_by(genres) %>%
    summarize(b_g=mean(rating-mu)) %>%
    ungroup()

# pub year bias
  pub_year_bias <- edx_work %>%
    left_join(x.,y=genre_bias,by="genres") %>%
    group_by(pub_year) %>%
    summarize(b_py=mean(rating-mu-b_g)) %>%
    ungroup()

# movie bias
  movie_bias <- edx_work %>%
    left_join(x.,y=genre_bias,by="genres") %>%
    left_join(x=.,y=pub_year_bias,by="pub_year") %>%
    group_by(movieId) %>%
    summarize(b_i=mean(rating-mu-b_g-b_py),n_rating=n_distinct(userId)) %>%
    ungroup()
  
# user bias
  user_bias <- edx_work %>%
    left_join(x.,y=genre_bias,by="genres") %>%
    left_join(x=.,y=pub_year_bias,by="pub_year") %>%
    left_join(x=.,y=movie_bias,by="movieId") %>%
    group_by(userId) %>%
    summarize(b_u=mean(rating-mu-b_g-b_py-b_i)) %>%
    ungroup()

# review week bias
  review_week_bias <- edx_work %>% 
    mutate(t= year(round_date(review_date, "year")) - 
             pub_year,review_week=round_date(review_date,"week")) %>%
    left_join(x.,y=genre_bias,by="genres") %>%
    left_join(x=.,y=pub_year_bias,by="pub_year") %>%
    left_join(x=.,y=movie_bias,by="movieId") %>%
    left_join(x=.,y=user_bias,by="userId") %>%
    group_by(review_week) %>%
    summarize(b_w=mean(rating-mu-b_g-b_py-b_i-b_u)) %>%
    ungroup()
  
# t bias
  t_bias <- edx_work %>% 
    mutate(t= year(round_date(review_date, "year")) - 
             pub_year,review_week=round_date(review_date,"week")) %>%
    left_join(x.,y=genre_bias,by="genres") %>%
    left_join(x=.,y=pub_year_bias,by="pub_year") %>%
    left_join(x=.,y=movie_bias,by="movieId") %>%
    left_join(x=.,y=user_bias,by="userId") %>%
    left_join(x=.,y=review_week_bias,by="review_week") %>%
    group_by(t) %>%
    summarize(b_t=mean(rating-mu-b_g-b_py-b_i-b_u-b_w)) %>%
    ungroup()

#define the training dataset:

# define the user-specific review order
  review_order <- edx_work %>%
    mutate(review_week=round_date(review_date,"week")) %>%
    select(userId,review_week) %>%
    distinct() %>%
    group_by(userId) %>%
    summarize(review_week,review_order=order(review_week))

# join to data set
  edx_work_order <- edx_work %>% 
    mutate(t= year(round_date(review_date, "year")) - 
             pub_year,review_week=round_date(review_date,"week")) %>%
    mutate(review_week=round_date(review_date,"week")) %>%
    left_join(x=.,y=review_order, by=c("userId","review_week"))

# complete the dataset
  edx_work_eln <- edx_work_order %>%
    left_join(x=.,y=movie_bias,by="movieId") %>%
    left_join(x=.,y=genre_bias,by="genres") %>%
    left_join(x=.,y=pub_year_bias,by="pub_year") %>%
    left_join(x=.,y=user_bias,by="userId") %>%
    left_join(x=.,y=review_week_bias,by="review_week") %>%
    left_join(x=.,y=t_bias,by="t") %>%
    select(rating,movieId,userId,b_g,b_py,b_i,b_u,b_w,b_t,
           n_rating,review_order,review_week)

validation <- read_csv("data/validation.csv")

validation_work <- validation %>% 
  mutate(review_date = as_datetime(timestamp),pub_year=as.numeric(str_match(str_match(title,pattern1),pattern2)))

review_order_validation<- validation_work %>%
  mutate(review_week=round_date(review_date,"week")) %>%
  select(userId,review_week) %>%
  distinct() %>%
  group_by(userId) %>%
  summarize(review_week,review_order=order(review_week))

# join to data set
validation_work_order <- validation_work %>% 
    mutate(t= year(round_date(review_date, "year")) - 
             pub_year,review_week=round_date(review_date,"week")) %>%
    mutate(review_week=round_date(review_date,"week")) %>%
    left_join(x=.,y=review_order_validation, by=c("userId","review_week"))
  
  validation_work_eln <- validation_work_order %>%
    left_join(x=.,y=movie_bias,by="movieId") %>%
    left_join(x=.,y=genre_bias,by="genres") %>%
    left_join(x=.,y=pub_year_bias,by="pub_year") %>%
    left_join(x=.,y=user_bias,by="userId") %>%
    left_join(x=.,y=review_week_bias,by="review_week") %>%
    left_join(x=.,y=t_bias,by="t") %>%
    select(rating,movieId,userId,b_g,b_py,b_i,b_u,b_w,b_t,
           n_rating,review_order,review_week)

# train final model
set.seed(1, sample.kind="Rounding")
eln_model_final <- train(rating ~ b_g + b_py + b_i + b_u + b_t + b_w + n_rating + review_order,
                   data = edx_work_eln,
                   method = "glmnet",
                   trControl = ctrl_pars,
                   metric = "RMSE",
                   family="gaussian",
                   tuneGrid = expand.grid(alpha =seq(0,1,0.166),lambda = seq(0.0001,1,length =100)))

#make prediciton ofn validation dataset
eln_predict_final <- predict(eln_model_final,newdata = validation_work_eln, na.action=na.pass)

# fill NAs with mu
eln_predict_final <- tibble(eln_predict_final) %>% mutate(eln_predict_final = ifelse(is.na(eln_predict_final) == TRUE, mu, eln_predict_final))

#calculate final rmse
rmse_final <- RMSE(validation_work_eln$rating,eln_predict_final$eln_predict_final)

```

The final model yields an RMSE of 0.86462 on the *validation* dataset. R-squared is around 0.35 and all bias parameters are of approximately equal immportance for the model, while *n_rating* and *review_order* do not contribute very much to model prediction.

```{r model_eval_final, eval=TRUE,echo=TRUE,message = FALSE,warning=FALSE,fig.cap="QQ-plot of residual of final model"}
# print rmse
rmse_final

#summarize final model and compare to eln_model_2
res_final <- resamples(list(eln_model_final=eln_model_final,eln_model_2=eln_model_2))
summary(res_final)
eln_model_final$bestTune

#give variable importance
varImp(eln_model_final)

```

# Conclusion
Exploratory data analysis on 70 % of the initial dataset yielded effects of the individual movie, its publication year and its genre. Especially older movies with a publication year preceding the creation of the database or their release for movie rental services seem to have on average a higher rating than the younger movies.  This can be interpreted as such, that only movies that were still popular (could get a high rating) at the time were actually included in movie rental or streaming services, while movies which were released at this time or later had not to go past this additional "quality check".
Additionally it was observed, that the number of years a review was submitted after movie publication and the individual taste of the user should as well be taken into account. This temporal effect which is not completely independent from the effect of the publication year described above can be explained similarly, only movies with high ratings will actually be viewed (and reviewed) years after their publication. A bias on the week the review was submitted was also included. Furthermore, the number of ratings a movie receives in total shows some effect on the rating, and the individual order (stratified by weeks) in which a user submits the reviews (reflecting an individual development of preference or rating scheme).
Since it was not clear whether individual parameters were fully independent and regularization was described as an essential recipe for success in the initial movielens competition in the [blog article](http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/) referenced in the text book, elastic net regularized generalized linear regression was selected as modelling framework. Generalized linear regression had the additional advantage that calculations were actually feasible with the available hardware given the size of the dataset. 
In the end the final model yields an RMSE of 0.86452 on the validation data set (10 % of the original data). A tuning hyperparameter *alpha* = 0.498 shows that an equal mixture of lasso and  ridge regularization with a low penalty (*lambda*= 1e-04) is suitable for the model. The very low R-Square of around 0.35 shows that the majority of variance in the data is not explained by the model. Therefore, next steps which are not part of the present work would be to explore also other modelling frameworks and to also explore classification in contrast to regression as the rating scheme in the movie lens data is not continuous and therefore might be better represented as a classification task.
